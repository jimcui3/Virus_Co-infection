{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc1e306-54d3-4b0b-9859-cf56ff8e03c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bfc54a",
   "metadata": {},
   "source": [
    "### This airPLS code is used for baseline removal of SERS spectra.\n",
    "\n",
    "This code is written by its original author, Yizeng Liang and Zhimin Zhang. We did not make any modifications and simply applied it to our SERS data.\n",
    "\n",
    "Reference:\n",
    "Z.-M. Zhang, S. Chen, and Y.-Z. Liang, Baseline correction using adaptive iteratively reweighted penalized least squares. Analyst 135 (5), 1138-1146 (2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36b02f4-78b4-4a3b-81d9-45b70428f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "airPLS.py Copyright 2014 Renato Lombardo - renato.lombardo@unipa.it\n",
    "Baseline correction using adaptive iteratively reweighted penalized least squares\n",
    "\n",
    "This program is a translation in python of the R source code of airPLS version 2.0\n",
    "by Yizeng Liang and Zhang Zhimin - https://code.google.com/p/airpls\n",
    "Reference:\n",
    "Z.-M. Zhang, S. Chen, and Y.-Z. Liang, Baseline correction using adaptive iteratively reweighted penalized least squares. Analyst 135 (5), 1138-1146 (2010).\n",
    "\n",
    "Description from the original documentation:\n",
    "\n",
    "Baseline drift always blurs or even swamps signals and deteriorates analytical results, particularly in multivariate analysis.  It is necessary to correct baseline drift to perform further data analysis. Simple or modified polynomial fitting has been found to be effective in some extent. However, this method requires user intervention and prone to variability especially in low signal-to-noise ratio environments. The proposed adaptive iteratively reweighted Penalized Least Squares (airPLS) algorithm doesn't require any user intervention and prior information, such as detected peaks. It iteratively changes weights of sum squares errors (SSE) between the fitted baseline and original signals, and the weights of SSE are obtained adaptively using between previously fitted baseline and original signals. This baseline estimator is general, fast and flexible in fitting baseline.\n",
    "\n",
    "\n",
    "LICENCE\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Lesser General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix, eye, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "def WhittakerSmooth(x,w,lambda_,differences=1):\n",
    "    '''\n",
    "    Penalized least squares algorithm for background fitting\n",
    "    \n",
    "    input\n",
    "        x: input data (i.e. chromatogram of spectrum)\n",
    "        w: binary masks (value of the mask is zero if a point belongs to peaks and one otherwise)\n",
    "        lambda_: parameter that can be adjusted by user. The larger lambda is,  the smoother the resulting background\n",
    "        differences: integer indicating the order of the difference of penalties\n",
    "    \n",
    "    output\n",
    "        the fitted background vector\n",
    "    '''\n",
    "    X=np.matrix(x)\n",
    "    m=X.size\n",
    "    E=eye(m,format='csc')\n",
    "    for i in range(differences):\n",
    "        E=E[1:]-E[:-1] # numpy.diff() does not work with sparse matrix. This is a workaround.\n",
    "    W=diags(w,0,shape=(m,m))\n",
    "    A=csc_matrix(W+(lambda_*E.T*E))\n",
    "    B=csc_matrix(W*X.T)\n",
    "    background=spsolve(A,B)\n",
    "    return np.array(background)\n",
    "\n",
    "def airPLS(x, lambda_=100, porder=1, itermax=15):\n",
    "    '''\n",
    "    Adaptive iteratively reweighted penalized least squares for baseline fitting\n",
    "    \n",
    "    input\n",
    "        x: input data (i.e. chromatogram of spectrum)\n",
    "        lambda_: parameter that can be adjusted by user. The larger lambda is,  the smoother the resulting background, z\n",
    "        porder: adaptive iteratively reweighted penalized least squares for baseline fitting\n",
    "    \n",
    "    output\n",
    "        the fitted background vector\n",
    "    '''\n",
    "    m=x.shape[0]\n",
    "    w=np.ones(m)\n",
    "    for i in range(1,itermax+1):\n",
    "        z=WhittakerSmooth(x,w,lambda_, porder)\n",
    "        d=x-z\n",
    "        dssn=np.abs(d[d<0].sum())\n",
    "        if(dssn<0.001*(abs(x)).sum() or i==itermax):\n",
    "            if(i==itermax): print('WARING max iteration reached!')\n",
    "            break\n",
    "        w[d>=0]=0 # d>0 means that this point is part of a peak, so its weight is set to 0 in order to ignore it\n",
    "        w[d<0]=np.exp(i*np.abs(d[d<0])/dssn)\n",
    "        w[0]=np.exp(i*(d[d<0]).max()/dssn) \n",
    "        w[-1]=w[0]\n",
    "    return z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d238f70",
   "metadata": {},
   "source": [
    "#### Batch processing using airPLS\n",
    "\n",
    "The airPLS code is applied to all csvs in the given root folder, and will save a processed csv for each original csv in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd862a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_normalization(x, y):\n",
    "    '''\n",
    "    Area normalization, ensure the area under the normalized spectra = 1\n",
    "    \n",
    "    input\n",
    "        x: wavenumber column\n",
    "        y: intensity column\n",
    "    \n",
    "    output\n",
    "        the normalized intensity column\n",
    "    '''\n",
    "    if np.any(np.isnan(x)) or np.any(np.isnan(y)) or np.any(np.isinf(x)) or np.any(np.isinf(y)):\n",
    "        print(\"Warning: NaN or inf values detected in the data\")\n",
    "        return y\n",
    "    area = integrate.trapz(y, x)\n",
    "    if area != 0:\n",
    "        return y / area\n",
    "    else:\n",
    "        print(\"Warning: Area is zero, returning original spectrum\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "879d847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CoVNL63-100000_FluB-100.csv\n",
      "Processing CoVNL63-100000_FluB-100000.csv\n",
      "Processing CoVNL63-100000_FluB-12500.csv\n",
      "Processing CoVNL63-100000_FluB-1562.csv\n",
      "Processing CoVNL63-100000_FluB-195.csv\n",
      "Processing CoVNL63-100000_FluB-25000.csv\n",
      "Processing CoVNL63-100000_FluB-3125.csv\n",
      "Processing CoVNL63-100000_FluB-391.csv\n",
      "Processing CoVNL63-100000_FluB-50.csv\n",
      "Processing CoVNL63-100000_FluB-50000.csv\n",
      "Processing CoVNL63-100000_FluB-6250.csv\n",
      "Processing CoVNL63-100000_FluB-781.csv\n",
      "Processing CoVNL63-100_FluB-100.csv\n",
      "Processing CoVNL63-100_FluB-100000.csv\n",
      "Processing CoVNL63-100_FluB-12500.csv\n",
      "Processing CoVNL63-100_FluB-1562.csv\n",
      "Processing CoVNL63-100_FluB-195.csv\n",
      "Processing CoVNL63-100_FluB-25000.csv\n",
      "Processing CoVNL63-100_FluB-3125.csv\n",
      "Processing CoVNL63-100_FluB-391.csv\n",
      "Processing CoVNL63-100_FluB-50.csv\n",
      "Processing CoVNL63-100_FluB-50000.csv\n",
      "Processing CoVNL63-100_FluB-6250.csv\n",
      "Processing CoVNL63-100_FluB-781.csv\n",
      "Processing CoVNL63-12500_FluB-100.csv\n",
      "Processing CoVNL63-12500_FluB-100000.csv\n",
      "Processing CoVNL63-12500_FluB-12500.csv\n",
      "Processing CoVNL63-12500_FluB-1562.csv\n",
      "Processing CoVNL63-12500_FluB-195.csv\n",
      "Processing CoVNL63-12500_FluB-25000.csv\n",
      "Processing CoVNL63-12500_FluB-3125.csv\n",
      "Processing CoVNL63-12500_FluB-391.csv\n",
      "Processing CoVNL63-12500_FluB-50.csv\n",
      "Processing CoVNL63-12500_FluB-50000.csv\n",
      "Processing CoVNL63-12500_FluB-6250.csv\n",
      "Processing CoVNL63-12500_FluB-781.csv\n",
      "Processing CoVNL63-1562_FluB-100.csv\n",
      "Processing CoVNL63-1562_FluB-100000.csv\n",
      "Processing CoVNL63-1562_FluB-12500.csv\n",
      "Processing CoVNL63-1562_FluB-1562.csv\n",
      "Processing CoVNL63-1562_FluB-195.csv\n",
      "Processing CoVNL63-1562_FluB-25000.csv\n",
      "Processing CoVNL63-1562_FluB-3125.csv\n",
      "Processing CoVNL63-1562_FluB-391.csv\n",
      "Processing CoVNL63-1562_FluB-50.csv\n",
      "Processing CoVNL63-1562_FluB-50000.csv\n",
      "Processing CoVNL63-1562_FluB-6250.csv\n",
      "Processing CoVNL63-1562_FluB-781.csv\n",
      "Processing CoVNL63-195_FluB-100.csv\n",
      "Processing CoVNL63-195_FluB-100000.csv\n",
      "Processing CoVNL63-195_FluB-12500.csv\n",
      "Processing CoVNL63-195_FluB-1562.csv\n",
      "Processing CoVNL63-195_FluB-195.csv\n",
      "Processing CoVNL63-195_FluB-25000.csv\n",
      "Processing CoVNL63-195_FluB-3125.csv\n",
      "Processing CoVNL63-195_FluB-391.csv\n",
      "Processing CoVNL63-195_FluB-50.csv\n",
      "Processing CoVNL63-195_FluB-50000.csv\n",
      "Processing CoVNL63-195_FluB-6250.csv\n",
      "Processing CoVNL63-195_FluB-781.csv\n",
      "Processing CoVNL63-25000_FluB-100.csv\n",
      "Processing CoVNL63-25000_FluB-100000.csv\n",
      "Processing CoVNL63-25000_FluB-12500.csv\n",
      "Processing CoVNL63-25000_FluB-1562.csv\n",
      "Processing CoVNL63-25000_FluB-195.csv\n",
      "Processing CoVNL63-25000_FluB-25000.csv\n",
      "Processing CoVNL63-25000_FluB-3125.csv\n",
      "Processing CoVNL63-25000_FluB-391.csv\n",
      "Processing CoVNL63-25000_FluB-50.csv\n",
      "Processing CoVNL63-25000_FluB-50000.csv\n",
      "Processing CoVNL63-25000_FluB-6250.csv\n",
      "Processing CoVNL63-25000_FluB-781.csv\n",
      "Processing CoVNL63-3125_FluB-100.csv\n",
      "Processing CoVNL63-3125_FluB-100000.csv\n",
      "Processing CoVNL63-3125_FluB-12500.csv\n",
      "Processing CoVNL63-3125_FluB-1562.csv\n",
      "Processing CoVNL63-3125_FluB-195.csv\n",
      "Processing CoVNL63-3125_FluB-25000.csv\n",
      "Processing CoVNL63-3125_FluB-3125.csv\n",
      "Processing CoVNL63-3125_FluB-391.csv\n",
      "Processing CoVNL63-3125_FluB-50.csv\n",
      "Processing CoVNL63-3125_FluB-50000.csv\n",
      "Processing CoVNL63-3125_FluB-6250.csv\n",
      "Processing CoVNL63-3125_FluB-781.csv\n",
      "Processing CoVNL63-391_FluB-100.csv\n",
      "Processing CoVNL63-391_FluB-100000.csv\n",
      "Processing CoVNL63-391_FluB-12500.csv\n",
      "Processing CoVNL63-391_FluB-1562.csv\n",
      "Processing CoVNL63-391_FluB-195.csv\n",
      "Processing CoVNL63-391_FluB-25000.csv\n",
      "Processing CoVNL63-391_FluB-3125.csv\n",
      "Processing CoVNL63-391_FluB-391.csv\n",
      "Processing CoVNL63-391_FluB-50.csv\n",
      "Processing CoVNL63-391_FluB-50000.csv\n",
      "Processing CoVNL63-391_FluB-6250.csv\n",
      "Processing CoVNL63-391_FluB-781.csv\n",
      "Processing CoVNL63-50000_FluB-100.csv\n",
      "Processing CoVNL63-50000_FluB-100000.csv\n",
      "Processing CoVNL63-50000_FluB-12500.csv\n",
      "Processing CoVNL63-50000_FluB-1562.csv\n",
      "Processing CoVNL63-50000_FluB-195.csv\n",
      "Processing CoVNL63-50000_FluB-25000.csv\n",
      "Processing CoVNL63-50000_FluB-3125.csv\n",
      "Processing CoVNL63-50000_FluB-391.csv\n",
      "Processing CoVNL63-50000_FluB-50.csv\n",
      "Processing CoVNL63-50000_FluB-50000.csv\n",
      "Processing CoVNL63-50000_FluB-6250.csv\n",
      "Processing CoVNL63-50000_FluB-781.csv\n",
      "Processing CoVNL63-50_FluB-100.csv\n",
      "Processing CoVNL63-50_FluB-100000.csv\n",
      "Processing CoVNL63-50_FluB-12500.csv\n",
      "Processing CoVNL63-50_FluB-1562.csv\n",
      "Processing CoVNL63-50_FluB-195.csv\n",
      "Processing CoVNL63-50_FluB-25000.csv\n",
      "Processing CoVNL63-50_FluB-3125.csv\n",
      "Processing CoVNL63-50_FluB-391.csv\n",
      "Processing CoVNL63-50_FluB-50.csv\n",
      "Processing CoVNL63-50_FluB-50000.csv\n",
      "Processing CoVNL63-50_FluB-6250.csv\n",
      "Processing CoVNL63-50_FluB-781.csv\n",
      "Processing CoVNL63-6250_FluB-100.csv\n",
      "Processing CoVNL63-6250_FluB-100000.csv\n",
      "Processing CoVNL63-6250_FluB-12500.csv\n",
      "Processing CoVNL63-6250_FluB-1562.csv\n",
      "Processing CoVNL63-6250_FluB-195.csv\n",
      "Processing CoVNL63-6250_FluB-25000.csv\n",
      "Processing CoVNL63-6250_FluB-3125.csv\n",
      "Processing CoVNL63-6250_FluB-391.csv\n",
      "Processing CoVNL63-6250_FluB-50.csv\n",
      "Processing CoVNL63-6250_FluB-50000.csv\n",
      "Processing CoVNL63-6250_FluB-6250.csv\n",
      "Processing CoVNL63-6250_FluB-781.csv\n",
      "Processing CoVNL63-781_FluB-100.csv\n",
      "Processing CoVNL63-781_FluB-100000.csv\n",
      "Processing CoVNL63-781_FluB-12500.csv\n",
      "Processing CoVNL63-781_FluB-1562.csv\n",
      "Processing CoVNL63-781_FluB-195.csv\n",
      "Processing CoVNL63-781_FluB-25000.csv\n",
      "Processing CoVNL63-781_FluB-3125.csv\n",
      "Processing CoVNL63-781_FluB-391.csv\n",
      "Processing CoVNL63-781_FluB-50.csv\n",
      "Processing CoVNL63-781_FluB-50000.csv\n",
      "Processing CoVNL63-781_FluB-6250.csv\n",
      "Processing CoVNL63-781_FluB-781.csv\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing the CSV files\n",
    "root_folder_path = r\"E:\\Yanjun Yang\\Virus in Saliva - Single detection (Original intensity data)\"\n",
    "output_folder_path = r\"E:\\Yanjun Yang\\airPLS removed\"\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for file_name in os.listdir(root_folder_path):\n",
    "    if file_name.endswith('.csv'):  # Check if the file is a CSV\n",
    "        print(\"Processing \" + file_name)\n",
    "        file_path = os.path.join(root_folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        x = df.iloc[:, 0].values\n",
    "\n",
    "        # Area normalization, ensure the area under the normalized spectra = 1\n",
    "        for column in df.columns[1:]:\n",
    "            df[column] = area_normalization(x, df[column].values)\n",
    "        \n",
    "        # Apply airPLS to every column except the first one (wavenumbers)\n",
    "        for col in df.columns[1:]:\n",
    "            baseline = airPLS(df[col].values, lambda_=90, porder=1, itermax=100)\n",
    "            df[col] = df[col].values - baseline\n",
    "        \n",
    "        # Do area normalization again to ensure that each output spectrum has the same scale\n",
    "        for column in df.columns[1:]:\n",
    "            df[column] = area_normalization(x, df[column].values)\n",
    "        \n",
    "        # Define the new file name\n",
    "        new_file_name = file_name.replace('.csv', '_airPLS_processed.csv')\n",
    "        new_file_path = os.path.join(output_folder_path, new_file_name)\n",
    "        \n",
    "        # Save the processed dataframe to a new CSV file\n",
    "        df.to_csv(new_file_path, sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e00c36-6434-4fda-bb85-9223b59f13c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
